{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a58f7cc",
   "metadata": {},
   "source": [
    "# Testing MobileNet Models on Rust Dataset\n",
    "\n",
    "This notebook tests all three MobileNet variants (V2, V3-Large, V3-Small) on the test images from the Rust_Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cdcd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_recall_fscore_support\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859b8492",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "IMG_SIZE = 640\n",
    "NUM_CLASSES = 4\n",
    "test_dir = \"../Rust_Dataset/test\"\n",
    "\n",
    "# Define transforms for test data\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load test dataset\n",
    "test_dataset = datasets.ImageFolder(test_dir, transform=test_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# Get class names\n",
    "class_names = test_dataset.classes\n",
    "print(f\"Classes: {class_names}\")\n",
    "print(f\"Number of test images: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454ed467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load model\n",
    "def load_model(model_name, model_path, num_classes=4):\n",
    "    \"\"\"Load a pre-trained MobileNet model with saved weights\"\"\"\n",
    "    \n",
    "    if model_name == 'mobilenet_v2':\n",
    "        model = models.mobilenet_v2(weights='IMAGENET1K_V1')\n",
    "        model.classifier[1] = nn.Linear(model.last_channel, num_classes)\n",
    "    elif model_name == 'mobilenet_v3_large':\n",
    "        model = models.mobilenet_v3_large(weights='IMAGENET1K_V1')\n",
    "        num_ftrs = model.classifier[3].in_features\n",
    "        model.classifier[3] = nn.Linear(num_ftrs, num_classes)\n",
    "    elif model_name == 'mobilenet_v3_small':\n",
    "        model = models.mobilenet_v3_small(weights='IMAGENET1K_V1')\n",
    "        num_ftrs = model.classifier[3].in_features\n",
    "        model.classifier[3] = nn.Linear(num_ftrs, num_classes)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model: {model_name}\")\n",
    "    \n",
    "    # Load trained weights\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    return model\n",
    "\n",
    "print(\"Model loading function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdb4437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate model\n",
    "def evaluate_model(model, test_loader, device):\n",
    "    \"\"\"Evaluate model and return predictions, true labels, and inference time\"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.numpy())\n",
    "    \n",
    "    end_time = time.time()\n",
    "    inference_time = ((end_time - start_time) / len(test_loader.dataset)) * 1000  # ms per image\n",
    "    \n",
    "    return np.array(all_preds), np.array(all_labels), inference_time\n",
    "\n",
    "print(\"Evaluation function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab275f9",
   "metadata": {},
   "source": [
    "## Test All Models\n",
    "\n",
    "Now we'll test all three MobileNet variants on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa5330c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models to test\n",
    "models_to_test = [\n",
    "    ('mobilenet_v2', 'rust_mobilenetv2.pth', 'MobileNet V2'),\n",
    "    ('mobilenet_v3_large', 'rust_mobilenetv3_large.pth', 'MobileNet V3 Large'),\n",
    "    ('mobilenet_v3_small', 'rust_mobilenetv3_small.pth', 'MobileNet V3 Small')\n",
    "]\n",
    "\n",
    "# Store results\n",
    "results = {}\n",
    "\n",
    "# Test each model\n",
    "for model_name, model_path, display_name in models_to_test:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Testing {display_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Load model\n",
    "    try:\n",
    "        model = load_model(model_name, model_path, NUM_CLASSES)\n",
    "        print(f\"✓ Model loaded successfully from {model_path}\")\n",
    "        \n",
    "        # Evaluate\n",
    "        predictions, true_labels, inf_time = evaluate_model(model, test_loader, device)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(true_labels, predictions)\n",
    "        precision, recall, f1, support = precision_recall_fscore_support(\n",
    "            true_labels, predictions, average='weighted', zero_division=0\n",
    "        )\n",
    "        \n",
    "        # Store results\n",
    "        results[display_name] = {\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'inference_time': inf_time,\n",
    "            'predictions': predictions,\n",
    "            'true_labels': true_labels,\n",
    "            'model_params': sum(p.numel() for p in model.parameters())\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nResults:\")\n",
    "        print(f\"  Accuracy:        {accuracy:.4f}\")\n",
    "        print(f\"  Precision:       {precision:.4f}\")\n",
    "        print(f\"  Recall:          {recall:.4f}\")\n",
    "        print(f\"  F1 Score:        {f1:.4f}\")\n",
    "        print(f\"  Inference Time:  {inf_time:.2f} ms/image\")\n",
    "        print(f\"  Total Parameters: {results[display_name]['model_params']:,}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error loading or testing {display_name}: {e}\")\n",
    "        results[display_name] = None\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Testing completed!\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12780fc2",
   "metadata": {},
   "source": [
    "## Detailed Classification Reports\n",
    "\n",
    "View detailed metrics for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7481f2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print detailed classification reports\n",
    "for model_name, result in results.items():\n",
    "    if result is not None:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Classification Report - {model_name}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(classification_report(\n",
    "            result['true_labels'], \n",
    "            result['predictions'],\n",
    "            target_names=class_names,\n",
    "            digits=4\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08573ad3",
   "metadata": {},
   "source": [
    "## Confusion Matrices\n",
    "\n",
    "Visualize the confusion matrices for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dade0b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrices\n",
    "num_models = sum(1 for r in results.values() if r is not None)\n",
    "fig, axes = plt.subplots(1, num_models, figsize=(6*num_models, 5))\n",
    "\n",
    "if num_models == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "idx = 0\n",
    "for model_name, result in results.items():\n",
    "    if result is not None:\n",
    "        cm = confusion_matrix(result['true_labels'], result['predictions'])\n",
    "        \n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                    xticklabels=class_names, yticklabels=class_names,\n",
    "                    ax=axes[idx])\n",
    "        axes[idx].set_title(f'{model_name}\\nConfusion Matrix')\n",
    "        axes[idx].set_ylabel('True Label')\n",
    "        axes[idx].set_xlabel('Predicted Label')\n",
    "        idx += 1\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff303e92",
   "metadata": {},
   "source": [
    "## Model Comparison\n",
    "\n",
    "Compare all models across different metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea17254c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison table\n",
    "import pandas as pd\n",
    "\n",
    "comparison_data = []\n",
    "for model_name, result in results.items():\n",
    "    if result is not None:\n",
    "        comparison_data.append({\n",
    "            'Model': model_name,\n",
    "            'Accuracy': f\"{result['accuracy']:.4f}\",\n",
    "            'Precision': f\"{result['precision']:.4f}\",\n",
    "            'Recall': f\"{result['recall']:.4f}\",\n",
    "            'F1 Score': f\"{result['f1']:.4f}\",\n",
    "            'Inference Time (ms)': f\"{result['inference_time']:.2f}\",\n",
    "            'Parameters': f\"{result['model_params']:,}\"\n",
    "        })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"MODEL COMPARISON SUMMARY\")\n",
    "print(\"=\"*100)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abb3315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Prepare data\n",
    "model_names = [r['Model'] for r in comparison_data]\n",
    "accuracies = [float(r['Accuracy']) for r in comparison_data]\n",
    "precisions = [float(r['Precision']) for r in comparison_data]\n",
    "recalls = [float(r['Recall']) for r in comparison_data]\n",
    "f1_scores = [float(r['F1 Score']) for r in comparison_data]\n",
    "inf_times = [float(r['Inference Time (ms)']) for r in comparison_data]\n",
    "\n",
    "# Accuracy comparison\n",
    "axes[0, 0].bar(model_names, accuracies, color=['#1f77b4', '#ff7f0e', '#2ca02c'])\n",
    "axes[0, 0].set_title('Accuracy Comparison', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_ylabel('Accuracy')\n",
    "axes[0, 0].set_ylim([0, 1])\n",
    "axes[0, 0].grid(axis='y', alpha=0.3)\n",
    "for i, v in enumerate(accuracies):\n",
    "    axes[0, 0].text(i, v + 0.02, f'{v:.4f}', ha='center', va='bottom')\n",
    "\n",
    "# Precision, Recall, F1 comparison\n",
    "x = np.arange(len(model_names))\n",
    "width = 0.25\n",
    "axes[0, 1].bar(x - width, precisions, width, label='Precision', color='#1f77b4')\n",
    "axes[0, 1].bar(x, recalls, width, label='Recall', color='#ff7f0e')\n",
    "axes[0, 1].bar(x + width, f1_scores, width, label='F1 Score', color='#2ca02c')\n",
    "axes[0, 1].set_title('Precision, Recall, F1 Score', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_ylabel('Score')\n",
    "axes[0, 1].set_xticks(x)\n",
    "axes[0, 1].set_xticklabels(model_names, rotation=15, ha='right')\n",
    "axes[0, 1].set_ylim([0, 1])\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Inference time comparison\n",
    "axes[1, 0].bar(model_names, inf_times, color=['#d62728', '#9467bd', '#8c564b'])\n",
    "axes[1, 0].set_title('Inference Time Comparison', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_ylabel('Time (ms/image)')\n",
    "axes[1, 0].grid(axis='y', alpha=0.3)\n",
    "for i, v in enumerate(inf_times):\n",
    "    axes[1, 0].text(i, v + 0.1, f'{v:.2f}', ha='center', va='bottom')\n",
    "\n",
    "# Parameters comparison\n",
    "params = [int(r['Parameters'].replace(',', '')) for r in comparison_data]\n",
    "axes[1, 1].bar(model_names, params, color=['#e377c2', '#7f7f7f', '#bcbd22'])\n",
    "axes[1, 1].set_title('Model Parameters', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].set_ylabel('Number of Parameters')\n",
    "axes[1, 1].grid(axis='y', alpha=0.3)\n",
    "axes[1, 1].ticklabel_format(style='plain', axis='y')\n",
    "for i, v in enumerate(params):\n",
    "    axes[1, 1].text(i, v + max(params)*0.02, f'{v:,}', ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed26a7f",
   "metadata": {},
   "source": [
    "## Best Model Summary\n",
    "\n",
    "Identify the best performing model for each metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20628d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best models for each metric\n",
    "metrics = ['accuracy', 'precision', 'recall', 'f1']\n",
    "metric_names = ['Accuracy', 'Precision', 'Recall', 'F1 Score']\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BEST MODELS BY METRIC\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for metric, metric_name in zip(metrics, metric_names):\n",
    "    best_model = max(results.items(), key=lambda x: x[1][metric] if x[1] is not None else -1)\n",
    "    if best_model[1] is not None:\n",
    "        print(f\"\\n{metric_name}:\")\n",
    "        print(f\"  Best Model: {best_model[0]}\")\n",
    "        print(f\"  Score: {best_model[1][metric]:.4f}\")\n",
    "\n",
    "# Best inference time (lowest)\n",
    "best_inf_model = min(results.items(), key=lambda x: x[1]['inference_time'] if x[1] is not None else float('inf'))\n",
    "if best_inf_model[1] is not None:\n",
    "    print(f\"\\nFastest Inference:\")\n",
    "    print(f\"  Best Model: {best_inf_model[0]}\")\n",
    "    print(f\"  Time: {best_inf_model[1]['inference_time']:.2f} ms/image\")\n",
    "\n",
    "# Most efficient (smallest)\n",
    "best_param_model = min(results.items(), key=lambda x: x[1]['model_params'] if x[1] is not None else float('inf'))\n",
    "if best_param_model[1] is not None:\n",
    "    print(f\"\\nSmallest Model:\")\n",
    "    print(f\"  Best Model: {best_param_model[0]}\")\n",
    "    print(f\"  Parameters: {best_param_model[1]['model_params']:,}\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
