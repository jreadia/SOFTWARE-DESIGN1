{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0232a1b",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a0eec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda64848",
   "metadata": {},
   "source": [
    "## Setup Device and Data Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15398210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "if device.type == 'cuda':\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
    "    print(f'Memory Available: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d899ff86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data transforms for testing\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a8a01c",
   "metadata": {},
   "source": [
    "## Load Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd64f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test dataset\n",
    "test_dir = 'Rust_Dataset/test'\n",
    "test_dataset = ImageFolder(test_dir, transform=test_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "\n",
    "# Get class names\n",
    "class_names = test_dataset.classes\n",
    "num_classes = len(class_names)\n",
    "\n",
    "print(f'Test Dataset Size: {len(test_dataset)}')\n",
    "print(f'Number of Classes: {num_classes}')\n",
    "print(f'Class Names: {class_names}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3a4ca4",
   "metadata": {},
   "source": [
    "## Define Model Loading Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094805b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_name, model_path):\n",
    "    \"\"\"Load a pre-trained model and its weights\"\"\"\n",
    "    if model_name == 'resnet50':\n",
    "        model = models.resnet50(pretrained=False)\n",
    "        model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "    elif model_name == 'densenet121':\n",
    "        model = models.densenet121(pretrained=False)\n",
    "        model.classifier = nn.Linear(model.classifier.in_features, num_classes)\n",
    "    elif model_name == 'efficientnet_b0':\n",
    "        model = models.efficientnet_b0(pretrained=False)\n",
    "        model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)\n",
    "    elif model_name == 'mobilenet_v2':\n",
    "        model = models.mobilenet_v2(pretrained=False)\n",
    "        model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)\n",
    "    elif model_name == 'mobilenet_v3_large':\n",
    "        model = models.mobilenet_v3_large(pretrained=False)\n",
    "        model.classifier[3] = nn.Linear(model.classifier[3].in_features, num_classes)\n",
    "    elif model_name == 'mobilenet_v3_small':\n",
    "        model = models.mobilenet_v3_small(pretrained=False)\n",
    "        model.classifier[3] = nn.Linear(model.classifier[3].in_features, num_classes)\n",
    "    else:\n",
    "        raise ValueError(f'Unknown model: {model_name}')\n",
    "    \n",
    "    # Load weights\n",
    "    if os.path.exists(model_path):\n",
    "        model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "        print(f'Loaded weights from {model_path}')\n",
    "    else:\n",
    "        print(f'Warning: Model weights not found at {model_path}')\n",
    "        return None\n",
    "    \n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e63638d",
   "metadata": {},
   "source": [
    "## Define Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bec0730",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_loader):\n",
    "    \"\"\"Evaluate model on test dataset\"\"\"\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(data_loader, desc='Testing'):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='weighted')\n",
    "    conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'confusion_matrix': conf_matrix,\n",
    "        'predictions': all_preds,\n",
    "        'labels': all_labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc897493",
   "metadata": {},
   "source": [
    "## Define Visualization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c51f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(conf_matrix, model_name):\n",
    "    \"\"\"Plot confusion matrix\"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title(f'Confusion Matrix - {model_name}')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b8adec",
   "metadata": {},
   "source": [
    "## Test All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52e2f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define all models to test\n",
    "models_to_test = [\n",
    "    ('ResNet50', 'resnet50', 'rust_resnet50.pth'),\n",
    "    ('DenseNet121', 'densenet121', 'rust_densenet121.pth'),\n",
    "    ('EfficientNetB0', 'efficientnet_b0', 'rust_efficientnetb0.pth'),\n",
    "    ('MobileNetV2', 'mobilenet_v2', 'rust_mobilenetv2.pth'),\n",
    "    ('MobileNetV3-Large', 'mobilenet_v3_large', 'mobile_net_versions/rust_mobilenetv3_large.pth'),\n",
    "    ('MobileNetV3-Small', 'mobilenet_v3_small', 'mobile_net_versions/rust_mobilenetv3_small.pth')\n",
    "]\n",
    "\n",
    "# Store results\n",
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42380368",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test each model\n",
    "for display_name, model_name, model_path in models_to_test:\n",
    "    print(f'\\n{\"=\"*60}')\n",
    "    print(f'Testing {display_name}')\n",
    "    print(f'{\"=\"*60}')\n",
    "    \n",
    "    # Load model\n",
    "    model = load_model(model_name, model_path)\n",
    "    \n",
    "    if model is None:\n",
    "        print(f'Skipping {display_name} due to loading error')\n",
    "        continue\n",
    "    \n",
    "    # Evaluate model\n",
    "    metrics = evaluate_model(model, test_loader)\n",
    "    results[display_name] = metrics\n",
    "    \n",
    "    # Print metrics\n",
    "    print(f'\\nResults for {display_name}:')\n",
    "    print(f'Accuracy:  {metrics[\"accuracy\"]*100:.2f}%')\n",
    "    print(f'Precision: {metrics[\"precision\"]*100:.2f}%')\n",
    "    print(f'Recall:    {metrics[\"recall\"]*100:.2f}%')\n",
    "    print(f'F1-Score:  {metrics[\"f1\"]*100:.2f}%')\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plot_confusion_matrix(metrics['confusion_matrix'], display_name)\n",
    "    \n",
    "    # Clean up memory\n",
    "    del model\n",
    "    torch.cuda.empty_cache() if torch.cuda.is_available() else None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d44a4dd",
   "metadata": {},
   "source": [
    "## Compare All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf911b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison table\n",
    "import pandas as pd\n",
    "\n",
    "comparison_data = []\n",
    "for model_name, metrics in results.items():\n",
    "    comparison_data.append({\n",
    "        'Model': model_name,\n",
    "        'Accuracy (%)': f\"{metrics['accuracy']*100:.2f}\",\n",
    "        'Precision (%)': f\"{metrics['precision']*100:.2f}\",\n",
    "        'Recall (%)': f\"{metrics['recall']*100:.2f}\",\n",
    "        'F1-Score (%)': f\"{metrics['f1']*100:.2f}\"\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print('\\n' + '='*80)\n",
    "print('MODEL COMPARISON SUMMARY')\n",
    "print('='*80)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print('='*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fda5c9b",
   "metadata": {},
   "source": [
    "## Visualize Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd771cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot comparison chart\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "model_names = list(results.keys())\n",
    "metrics_to_plot = ['accuracy', 'precision', 'recall', 'f1']\n",
    "x = np.arange(len(model_names))\n",
    "width = 0.2\n",
    "\n",
    "for i, metric in enumerate(metrics_to_plot):\n",
    "    values = [results[model][metric]*100 for model in model_names]\n",
    "    ax.bar(x + i*width, values, width, label=metric.capitalize())\n",
    "\n",
    "ax.set_xlabel('Models')\n",
    "ax.set_ylabel('Score (%)')\n",
    "ax.set_title('Model Performance Comparison')\n",
    "ax.set_xticks(x + width * 1.5)\n",
    "ax.set_xticklabels(model_names, rotation=45, ha='right')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6309e1b",
   "metadata": {},
   "source": [
    "## Per-Class Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b7af6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze per-class performance for each model\n",
    "for model_name, metrics in results.items():\n",
    "    print(f'\\n{\"=\"*60}')\n",
    "    print(f'Per-Class Metrics for {model_name}')\n",
    "    print(f'{\"=\"*60}')\n",
    "    \n",
    "    precision, recall, f1, support = precision_recall_fscore_support(\n",
    "        metrics['labels'], \n",
    "        metrics['predictions'],\n",
    "        labels=range(num_classes)\n",
    "    )\n",
    "    \n",
    "    class_metrics = pd.DataFrame({\n",
    "        'Class': class_names,\n",
    "        'Precision (%)': [f\"{p*100:.2f}\" for p in precision],\n",
    "        'Recall (%)': [f\"{r*100:.2f}\" for r in recall],\n",
    "        'F1-Score (%)': [f\"{f*100:.2f}\" for f in f1],\n",
    "        'Support': support\n",
    "    })\n",
    "    \n",
    "    print(class_metrics.to_string(index=False))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4ff8e5",
   "metadata": {},
   "source": [
    "## Identify Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f3e1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best model based on different metrics\n",
    "best_accuracy = max(results.items(), key=lambda x: x[1]['accuracy'])\n",
    "best_precision = max(results.items(), key=lambda x: x[1]['precision'])\n",
    "best_recall = max(results.items(), key=lambda x: x[1]['recall'])\n",
    "best_f1 = max(results.items(), key=lambda x: x[1]['f1'])\n",
    "\n",
    "print('\\n' + '='*60)\n",
    "print('BEST PERFORMING MODELS')\n",
    "print('='*60)\n",
    "print(f'Best Accuracy:  {best_accuracy[0]} ({best_accuracy[1][\"accuracy\"]*100:.2f}%)')\n",
    "print(f'Best Precision: {best_precision[0]} ({best_precision[1][\"precision\"]*100:.2f}%)')\n",
    "print(f'Best Recall:    {best_recall[0]} ({best_recall[1][\"recall\"]*100:.2f}%)')\n",
    "print(f'Best F1-Score:  {best_f1[0]} ({best_f1[1][\"f1\"]*100:.2f}%)')\n",
    "print('='*60)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
